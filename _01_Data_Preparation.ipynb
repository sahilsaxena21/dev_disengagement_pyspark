{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "_01_Data Preparation.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiYB9-VmYRc2"
      },
      "source": [
        "# 01 Introduction and Data Preparation - GitHub Developer Engagement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4p2SzFRYRc6"
      },
      "source": [
        "# I - Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HxC0cqJYRc7"
      },
      "source": [
        "## A: Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGvDr6EZYRc7"
      },
      "source": [
        "The Github open source community provides a unique opportunity to both businesses and the independent developers. To the businesses it provides a means to open the software and other products to a large base of inspectors who can quickly detect issues and provide feedback. To the independent developers, the open source platform provides a means to improve one’s technical skillset, work with more experienced practitioner and grow their professional network. \n",
        "\n",
        "The success of this platform depends on the employees of the organizations to maintain the Github repositories. These employees play a variety of roles, such as attend to code issues, perform code reviews, manage pull requests and perform commits. Maintaining a good work environment for these employees is important to secure the employee’s engagement in a sustainable way.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTMpswGFYRc9"
      },
      "source": [
        "## B: Aim Of This Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFCD0tGZYRc9"
      },
      "source": [
        "This analysis looks to study the effect of potential predictor variables i) workload related factors (i.e. number of commits by employee in their initial months) and ii) the stability of the work environment (i.e. work staff changes in the employee’s initial months) and its impact on employee engagement/disengagement. We consider an employee is “possibly_disengaged”, if we see a drastic reduction in their number of commits in the future months compared to their participation in the beginning months. If we see no such drastic reduction, we label those employees as “no_signs”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK7_OMiiYRc-"
      },
      "source": [
        "## C: Hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IDBy3TuYRc-"
      },
      "source": [
        "In terms of workload related factors, it is hypothesized that the magnitude of workload in the beginning months predicts future disengagement. For example, over-worked employees (i.e. performing a very high number of commits) in the beginning months would tend to disengage in the future months. \n",
        "\n",
        "In terms of work environment stability related factors, it is hypothesized that frequent changes in the committer team personnel to manage a repo contributes to an unstable work environment and predicts future employee disengagement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_-C8adNYRc_"
      },
      "source": [
        "## D: Selected Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxATTJBXYRc_"
      },
      "source": [
        "The analysis of this project looks to replicate the analysis of the selected paper (refer Section 6 for citation). Instead of Github commit activity, the selected paper looks into the activity summary reports of the developers to predict if the developer will become disengaged, and eventually leave the company. The selected paper considers “factors” (i.e. features) such as working hours (i.e. as a proxy of magnitude of workload), the nature of the projects the employee has worked on, and overall contribution statistics (e.g. number of projects).  The paper tests various classification models such as Random Forest, SVM etc. to predict if the employee will leave the company within the next 3 years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85YoS65FYRdA"
      },
      "source": [
        "# II - Initial Set-up of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ns6_rQYRdA"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Bi6jwDl8YRdA"
      },
      "source": [
        "# please note that it may take upto 15 min to run the whole notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "StZZYReXYRdB"
      },
      "source": [
        "import os\n",
        "os.environ[\"HADOOP_HOME\"] = \"S:/Courses/MIE1512/Material/Spark/winutils/\"\n",
        "os.environ[\"JAVA_HOME\"] = \"C:/progra~2/Java/jdk1.8.0_221/\"\n",
        "os.environ[\"JRE_HOME\"] = \"C:/progra~2/Java/jdk1.8.0_221/jre/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XyNYP8rXYRdB"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"S:/Courses/MIE1512/Material/Spark/spark-2.3.2-bin-hadoop2.7/\")\n",
        "\n",
        "import pyspark\n",
        "sc = pyspark.SparkContext(appName=\"myAppName\")\n",
        "spark = pyspark.sql.SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "udftpyBvYRdB"
      },
      "source": [
        "import pyspark.sql.types as T \n",
        "import pyspark.sql.functions as F\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "28nsz0TpYRdC"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nRSialImYRdC"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, ClusteringEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yIufsJslYRdC"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, ClusteringEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeB91ioAYRdD"
      },
      "source": [
        "### Load and clean csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BGrpFLoGYRdE"
      },
      "source": [
        "#load the linux csv\n",
        "commits_df_raw = spark.read.csv(\"commits_apache_2.csv\",\n",
        "                           header=True,\n",
        "                           inferSchema=True)\n",
        "commits_df_raw.createOrReplaceTempView(\"commits_raw\")\n",
        "\n",
        "all_table = spark.sql(\"\"\"\n",
        "SELECT * FROM commits_raw\n",
        "\"\"\")\n",
        "all_table.createOrReplaceTempView(\"repos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Mgb7kYOwYRdE",
        "outputId": "e2019703-7230-4639-e087-71b131c691a5"
      },
      "source": [
        "#check schema\n",
        "all_table.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- committer_name: string (nullable = true)\n",
            " |-- author_timestamp: string (nullable = true)\n",
            " |-- author_name: string (nullable = true)\n",
            " |-- repo_names: string (nullable = true)\n",
            " |-- committer_timestamp: string (nullable = true)\n",
            " |-- repo_org: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IusRK9xuYRdF",
        "outputId": "ae505a9d-c473-4e08-ba9b-e9aaff363e87"
      },
      "source": [
        "# check that it loaded properly\n",
        "all_table.limit(5).toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>committer_name</th>\n",
              "      <th>author_timestamp</th>\n",
              "      <th>author_name</th>\n",
              "      <th>repo_names</th>\n",
              "      <th>committer_timestamp</th>\n",
              "      <th>repo_org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Karl Wright</td>\n",
              "      <td>2013-09-18 20:48:59 UTC</td>\n",
              "      <td>Karl Wright</td>\n",
              "      <td>apache/manifoldcf</td>\n",
              "      <td>2013-09-18 20:48:59 UTC</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zheng Zhong</td>\n",
              "      <td>2006-02-23 18:06:50 UTC</td>\n",
              "      <td>Zheng Zhong</td>\n",
              "      <td>apache/portals-pluto</td>\n",
              "      <td>2006-02-23 18:06:50 UTC</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Simone Tripodi</td>\n",
              "      <td>2010-03-23 19:58:08 UTC</td>\n",
              "      <td>Simone Tripodi</td>\n",
              "      <td>apache/bval</td>\n",
              "      <td>2010-03-23 19:58:08 UTC</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stian Soiland-Reyes</td>\n",
              "      <td>2015-02-06 16:40:39 UTC</td>\n",
              "      <td>Stian Soiland-Reyes</td>\n",
              "      <td>apache/incubator-taverna-maven-parent</td>\n",
              "      <td>2015-02-06 16:40:39 UTC</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Paul Dick</td>\n",
              "      <td>2001-04-03 22:12:24 UTC</td>\n",
              "      <td>Paul Dick</td>\n",
              "      <td>apache/xalan-c</td>\n",
              "      <td>2001-04-03 22:12:24 UTC</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        committer_name         author_timestamp          author_name  \\\n",
              "0          Karl Wright  2013-09-18 20:48:59 UTC          Karl Wright   \n",
              "1          Zheng Zhong  2006-02-23 18:06:50 UTC          Zheng Zhong   \n",
              "2       Simone Tripodi  2010-03-23 19:58:08 UTC       Simone Tripodi   \n",
              "3  Stian Soiland-Reyes  2015-02-06 16:40:39 UTC  Stian Soiland-Reyes   \n",
              "4            Paul Dick  2001-04-03 22:12:24 UTC            Paul Dick   \n",
              "\n",
              "                              repo_names      committer_timestamp repo_org  \n",
              "0                      apache/manifoldcf  2013-09-18 20:48:59 UTC     None  \n",
              "1                   apache/portals-pluto  2006-02-23 18:06:50 UTC     None  \n",
              "2                            apache/bval  2010-03-23 19:58:08 UTC     None  \n",
              "3  apache/incubator-taverna-maven-parent  2015-02-06 16:40:39 UTC     None  \n",
              "4                         apache/xalan-c  2001-04-03 22:12:24 UTC     None  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glZQ1O13YRdG"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "vc_nDlmSYRdG"
      },
      "source": [
        "Two steps are performed:\n",
        "1. convert data type to timestamp\n",
        "2. add a column for the organization of the repo. These are all characters before the / in repo_names column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2rJDVFwJYRdG"
      },
      "source": [
        "# this function removes the \"UTC\" that we notice in author/committer_timestamp content\n",
        "def remove_last_chars(col):\n",
        "    new_col = col[:-4]\n",
        "    return new_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3PpPmL9BYRdG",
        "outputId": "be9e3c14-d199-4466-8bd7-c95c94d6aeea"
      },
      "source": [
        "#define a UDF and apply UDF\n",
        "udf_remove_last_chars = udf(remove_last_chars, StringType())\n",
        "all_table = all_table.withColumn(\"committer_timestamp\", udf_remove_last_chars(all_table[\"committer_timestamp\"]))\n",
        "all_table = all_table.withColumn(\"author_timestamp\", udf_remove_last_chars(all_table[\"author_timestamp\"]))\n",
        "all_table.limit(5).toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>committer_name</th>\n",
              "      <th>author_timestamp</th>\n",
              "      <th>author_name</th>\n",
              "      <th>repo_names</th>\n",
              "      <th>committer_timestamp</th>\n",
              "      <th>repo_org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Karl Wright</td>\n",
              "      <td>2013-09-18 20:48:59</td>\n",
              "      <td>Karl Wright</td>\n",
              "      <td>apache/manifoldcf</td>\n",
              "      <td>2013-09-18 20:48:59</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zheng Zhong</td>\n",
              "      <td>2006-02-23 18:06:50</td>\n",
              "      <td>Zheng Zhong</td>\n",
              "      <td>apache/portals-pluto</td>\n",
              "      <td>2006-02-23 18:06:50</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Simone Tripodi</td>\n",
              "      <td>2010-03-23 19:58:08</td>\n",
              "      <td>Simone Tripodi</td>\n",
              "      <td>apache/bval</td>\n",
              "      <td>2010-03-23 19:58:08</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stian Soiland-Reyes</td>\n",
              "      <td>2015-02-06 16:40:39</td>\n",
              "      <td>Stian Soiland-Reyes</td>\n",
              "      <td>apache/incubator-taverna-maven-parent</td>\n",
              "      <td>2015-02-06 16:40:39</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Paul Dick</td>\n",
              "      <td>2001-04-03 22:12:24</td>\n",
              "      <td>Paul Dick</td>\n",
              "      <td>apache/xalan-c</td>\n",
              "      <td>2001-04-03 22:12:24</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        committer_name     author_timestamp          author_name  \\\n",
              "0          Karl Wright  2013-09-18 20:48:59          Karl Wright   \n",
              "1          Zheng Zhong  2006-02-23 18:06:50          Zheng Zhong   \n",
              "2       Simone Tripodi  2010-03-23 19:58:08       Simone Tripodi   \n",
              "3  Stian Soiland-Reyes  2015-02-06 16:40:39  Stian Soiland-Reyes   \n",
              "4            Paul Dick  2001-04-03 22:12:24            Paul Dick   \n",
              "\n",
              "                              repo_names  committer_timestamp repo_org  \n",
              "0                      apache/manifoldcf  2013-09-18 20:48:59     None  \n",
              "1                   apache/portals-pluto  2006-02-23 18:06:50     None  \n",
              "2                            apache/bval  2010-03-23 19:58:08     None  \n",
              "3  apache/incubator-taverna-maven-parent  2015-02-06 16:40:39     None  \n",
              "4                         apache/xalan-c  2001-04-03 22:12:24     None  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1NYI89DwYRdH"
      },
      "source": [
        "#Cast as timestamp\n",
        "all_table = spark.sql(\"\"\"\n",
        "SELECT committer_name, \n",
        "CAST(unix_timestamp(committer_timestamp) as timestamp) as committer_timestamp,\n",
        "CAST(unix_timestamp(author_timestamp) as timestamp) as author_timestamp,\n",
        "author_name, repo_names\n",
        "FROM repos\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tevoLUh0YRdH"
      },
      "source": [
        "# this function creates a new column i.e. the organization the repo belongs to\n",
        "def create_repo_org(col):\n",
        "    new_col = col.split(\"/\")[0]\n",
        "    return new_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kZ-Con4iYRdH"
      },
      "source": [
        "#define a UDF and apply UDF\n",
        "udf_create_repo_org = udf(create_repo_org, StringType())\n",
        "all_table = all_table.withColumn(\"repo_org\", udf_create_repo_org(all_table[\"repo_names\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "znne-uXPYRdI",
        "outputId": "283ad55a-a085-4925-ceb2-2330231cb137"
      },
      "source": [
        "#check schema\n",
        "all_table.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- committer_name: string (nullable = true)\n",
            " |-- committer_timestamp: timestamp (nullable = true)\n",
            " |-- author_timestamp: timestamp (nullable = true)\n",
            " |-- author_name: string (nullable = true)\n",
            " |-- repo_names: string (nullable = true)\n",
            " |-- repo_org: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_0AWxtxtYRdI",
        "outputId": "def83363-e46a-4dad-ed3b-fb7fc4e0e468"
      },
      "source": [
        "all_table.limit(5).toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_timestamp</th>\n",
              "      <th>author_timestamp</th>\n",
              "      <th>author_name</th>\n",
              "      <th>repo_names</th>\n",
              "      <th>repo_org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Karl Wright</td>\n",
              "      <td>2013-09-18 20:48:59</td>\n",
              "      <td>2013-09-18 20:48:59</td>\n",
              "      <td>Karl Wright</td>\n",
              "      <td>apache/manifoldcf</td>\n",
              "      <td>apache</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zheng Zhong</td>\n",
              "      <td>2006-02-23 18:06:50</td>\n",
              "      <td>2006-02-23 18:06:50</td>\n",
              "      <td>Zheng Zhong</td>\n",
              "      <td>apache/portals-pluto</td>\n",
              "      <td>apache</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Simone Tripodi</td>\n",
              "      <td>2010-03-23 19:58:08</td>\n",
              "      <td>2010-03-23 19:58:08</td>\n",
              "      <td>Simone Tripodi</td>\n",
              "      <td>apache/bval</td>\n",
              "      <td>apache</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stian Soiland-Reyes</td>\n",
              "      <td>2015-02-06 16:40:39</td>\n",
              "      <td>2015-02-06 16:40:39</td>\n",
              "      <td>Stian Soiland-Reyes</td>\n",
              "      <td>apache/incubator-taverna-maven-parent</td>\n",
              "      <td>apache</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Paul Dick</td>\n",
              "      <td>2001-04-03 22:12:24</td>\n",
              "      <td>2001-04-03 22:12:24</td>\n",
              "      <td>Paul Dick</td>\n",
              "      <td>apache/xalan-c</td>\n",
              "      <td>apache</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        committer_name committer_timestamp    author_timestamp  \\\n",
              "0          Karl Wright 2013-09-18 20:48:59 2013-09-18 20:48:59   \n",
              "1          Zheng Zhong 2006-02-23 18:06:50 2006-02-23 18:06:50   \n",
              "2       Simone Tripodi 2010-03-23 19:58:08 2010-03-23 19:58:08   \n",
              "3  Stian Soiland-Reyes 2015-02-06 16:40:39 2015-02-06 16:40:39   \n",
              "4            Paul Dick 2001-04-03 22:12:24 2001-04-03 22:12:24   \n",
              "\n",
              "           author_name                             repo_names repo_org  \n",
              "0          Karl Wright                      apache/manifoldcf   apache  \n",
              "1          Zheng Zhong                   apache/portals-pluto   apache  \n",
              "2       Simone Tripodi                            apache/bval   apache  \n",
              "3  Stian Soiland-Reyes  apache/incubator-taverna-maven-parent   apache  \n",
              "4            Paul Dick                         apache/xalan-c   apache  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZacDVLGyYRdI"
      },
      "source": [
        "#save as temptable repos.\n",
        "all_table.createOrReplaceTempView(\"repos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ipzmmxYRdJ"
      },
      "source": [
        "# III - Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBbAevCEYRdJ"
      },
      "source": [
        "1. <b>first_commit and period_N: </b> \n",
        "\n",
        "    first_commit: This column contains the timestamp of the first commit by the developer towards the organization (i.e. Apple). It is used to calculate the periods relative to the developer (see period_N). \n",
        "\n",
        "    period_N: These are all type timestamp, calculated using formula first_commit + N, where N is an integer value from 1-6 (inclusive). Each N spans 4 months e.g. period_2 = first_commit + 8 months. The analysis looks into the commit activity for the first 2 years relative to the developer's first_commit i.e. till N = 6. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOT7_Gs-YRdJ"
      },
      "source": [
        "2. <b>P(N)_ numb_commits_org:</b>  Number of commits by the developer for each period for the organization. It is hypothesized that people who are over-worked (i.e. perform a high number of commits) in baseline period (i.e. first period, see 15. Y-Variable for more info) tend to disengage in future periods. This is adapted version of the feature p{N}_ hour_ sum in the paper. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1tLOIyDYRdJ"
      },
      "source": [
        "3. <b>P(N)_ numb_commits_repo:</b> Number of commits by the developer for each period for a given repo. This feature is used to calculate P(N)_ multi_ratio.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4_xg6VEYRdJ"
      },
      "source": [
        "4. <b>P(N)_ repo_comm_absent:</b> Total number of absentees for the repo over all periods. It is hypothesized that frequent changes in the committer team personnel for a repo from one period to another contributes to an unstable work environment and predicts future developer disengagement. This is adapted version of the feature \n",
        "p(N)_ person_change in the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-XR0liYRdK"
      },
      "source": [
        "5. <b>P(N)_ repo_comm_new:</b> Total number of new committers for the repo over all periods. It is hypothesized that frequent changes in the committer team personnel for a repo from one period to another contributes to an unstable work environment and predicts future developer disengagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSC_WZpGYRdK"
      },
      "source": [
        "6. <b>P(N)_ repo_committers: </b>Number of developers committing to the repo during each period. Similar to the stance of the paper, the number of project members is an indicator of the project size. Small project size usually means more workload to each individual in the project, thereby predicting future developer disengagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUWn9J9oYRdK"
      },
      "source": [
        "7. <b>total_repo_comm_absent:</b> For each successive period, the number of committers who committed for the previous period to the repo, but did not commit in the next period. E.g. P(1)_ repo_comm_absent represents the comparison of period_2 to period_1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8-s60MJYRdK"
      },
      "source": [
        "8. <b>total_repo_comm_new:</b> For each successive period, the number of committers who did not commit in the previous period to the repo, but committed in the next period. E.g. P(1)_repo_comm_new represents the comparison of period_2 to period_1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRKfrbPYRdL"
      },
      "source": [
        "9. <b>agg_absent_periods:</b> Number of periods in which the value of P(n)_repo_comm_absent is > 0. This is an aggregate form of P(n)_repo_comm_absent. This is the adapted version of feature in the paper. This is adapted version of the feature less_zero in the paper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olnsR9mhYRdL"
      },
      "source": [
        "10. <b>new_ppl_periods:</b> Number of periods in which the value of P(n)_repo_comm_new is > 0. This is an aggregate form of P(n)_repo_comm_new. This is the adapted version of feature in the paper. This is adapted version of the feature larger_zero in the paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AVU78jKYRdM"
      },
      "source": [
        "11. <b>no_change_periods:</b> Number of periods in which the value of P(n)_repo_comm_new & P(n)_repo_comm_absent are 0. It is hypothesized that the larger this number, the more sense of work environment stability, which thereby supports develepor engagement. This is adapted version of the feature equal_zero in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKJxhBifYRdM"
      },
      "source": [
        "12. <b>P(n)_ multi_ratio:</b> Ratio of commits between a repo to other repos for each period within the organization. It is calculated as: P(N)_numb_commit_repos / P(N)_numb_commits_org. It is used to calculate avg_multi_ratio. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue1TzhufYRdN"
      },
      "source": [
        "13. <b>total_commits_org:</b> This feature was removed from further analysis during V2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8pOC63pYRdN"
      },
      "source": [
        "14. <b>avg_multi_ratio:</b> Average ratio of commits between a repo to other repos within the organization over the first four periods. It is calculated as average of P(N)_multi_ratio over all of the six periods. This is adapted version of the feature multi_project in the paper. Similar to the paper, contributing to multiple repos is like contributing to “multiple projects (which) might mean higher workload”. It is hypothesized that developers facing high-workload tend to disengage in the future. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSOWc63NYRdO"
      },
      "source": [
        "15. <b> Y-Variable:</b> The future employee engagement i.e. the y-variable, is calculated as follows: \n",
        "\n",
        "   1. Number of commits between first_commit and period_1 is considered the baseline.\n",
        "   \n",
        "   2. If for both periods period_5 and period_6, the number of commits for a period is reduced by over 80% of the baseline, then the person     is “possibly_disengaged”, else the person is labelled as “no_signs” since they do not show signs of disengagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQiwt4p6YRdO"
      },
      "source": [
        "### 1. first_commit and period_N"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KeuR-DgBYRdO"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "SELECT row_number() over (order by committer_name ASC, committer_timestamp ASC, repo_names ASC) as ID,\n",
        "committer_name, committer_timestamp, repo_names, repo_org, \n",
        "MIN(committer_timestamp) over (partition by committer_name, repo_org) as first_commit\n",
        "FROM repos\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OO2eTr22YRdP",
        "outputId": "f840744c-68e6-41ac-c3c4-17bd6d171d30"
      },
      "source": [
        "all_table.limit(2).toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_timestamp</th>\n",
              "      <th>repo_names</th>\n",
              "      <th>repo_org</th>\n",
              "      <th>first_commit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Erik \"\"Ealanrian\"\" Jansman\"</td>\n",
              "      <td>2016-07-11 08:38:37</td>\n",
              "      <td>apache/celix</td>\n",
              "      <td>apache</td>\n",
              "      <td>2016-07-11 08:38:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>\"James \"\"Chuck\"\" Williams\"</td>\n",
              "      <td>2006-06-08 22:38:37</td>\n",
              "      <td>apache/axis2-java</td>\n",
              "      <td>apache</td>\n",
              "      <td>2006-06-08 22:38:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                committer_name committer_timestamp         repo_names  \\\n",
              "0   1  \"Erik \"\"Ealanrian\"\" Jansman\" 2016-07-11 08:38:37       apache/celix   \n",
              "1   2    \"James \"\"Chuck\"\" Williams\" 2006-06-08 22:38:37  apache/axis2-java   \n",
              "\n",
              "  repo_org        first_commit  \n",
              "0   apache 2016-07-11 08:38:37  \n",
              "1   apache 2006-06-08 22:38:37  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QzUHkUAMYRdP",
        "outputId": "d69ca7a6-0ec3-4186-bc09-87e436b3f5a4"
      },
      "source": [
        "all_table.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- committer_name: string (nullable = true)\n",
            " |-- committer_timestamp: timestamp (nullable = true)\n",
            " |-- repo_names: string (nullable = true)\n",
            " |-- repo_org: string (nullable = true)\n",
            " |-- first_commit: timestamp (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qNAFsg8dYRdP"
      },
      "source": [
        "#note that period_1 is the baseline period\n",
        "all_table = all_table.withColumn('period_1', all_table.first_commit + F.expr('INTERVAL 4 MONTH'))\n",
        "\n",
        "all_table = all_table.withColumn('period_2', all_table.first_commit + F.expr('INTERVAL 8 MONTH'))\n",
        "all_table = all_table.withColumn('period_3', all_table.first_commit + F.expr('INTERVAL 12 MONTH'))\n",
        "all_table = all_table.withColumn('period_4', all_table.first_commit + F.expr('INTERVAL 16 MONTH'))\n",
        "all_table = all_table.withColumn('period_5', all_table.first_commit + F.expr('INTERVAL 20 MONTH'))\n",
        "all_table = all_table.withColumn('period_6', all_table.first_commit + F.expr('INTERVAL 24 MONTH'))\n",
        "\n",
        "#below not included\n",
        "all_table = all_table.withColumn('period_7', all_table.first_commit + F.expr('INTERVAL 28 MONTH'))\n",
        "all_table = all_table.withColumn('period_8', all_table.first_commit + F.expr('INTERVAL 32 MONTH'))\n",
        "all_table = all_table.withColumn('period_9', all_table.first_commit + F.expr('INTERVAL 36 MONTH'))\n",
        "all_table = all_table.withColumn('period_10', all_table.first_commit + F.expr('INTERVAL 40 MONTH'))\n",
        "all_table = all_table.withColumn('period_11', all_table.first_commit + F.expr('INTERVAL 44 MONTH'))\n",
        "\n",
        "# for last_months\n",
        "all_table = all_table.withColumn('period_12', all_table.first_commit + F.expr('INTERVAL 12 MONTH'))\n",
        "all_table = all_table.withColumn('period_18', all_table.first_commit + F.expr('INTERVAL 18 MONTH'))\n",
        "\n",
        "# A trial and error process was taken up in V1 to define suitable y-variable. \n",
        "# The months below are created but are not used in ML model. However, they may still appear in tables below. \n",
        "# This will be cleaned up in V2.\n",
        "all_table = all_table.withColumn('period_25', all_table.first_commit + F.expr('INTERVAL 25 MONTH'))\n",
        "all_table = all_table.withColumn('period_32', all_table.first_commit + F.expr('INTERVAL 32 MONTH'))\n",
        "all_table = all_table.withColumn('period_33', all_table.first_commit + F.expr('INTERVAL 33 MONTH'))\n",
        "all_table = all_table.withColumn('period_34', all_table.first_commit + F.expr('INTERVAL 34 MONTH'))\n",
        "all_table = all_table.withColumn('period_35', all_table.first_commit + F.expr('INTERVAL 35 MONTH'))\n",
        "all_table = all_table.withColumn('period_36', all_table.first_commit + F.expr('INTERVAL 36 MONTH'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "I-Tcx1Z4YRdQ"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7063_jCSYRdQ",
        "outputId": "c1304e89-09c8-4459-8e4d-52ee99896fbf"
      },
      "source": [
        "all_table.limit(2).toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>committer_name</th>\n",
              "      <th>committer_timestamp</th>\n",
              "      <th>repo_names</th>\n",
              "      <th>repo_org</th>\n",
              "      <th>first_commit</th>\n",
              "      <th>period_1</th>\n",
              "      <th>period_2</th>\n",
              "      <th>period_3</th>\n",
              "      <th>period_4</th>\n",
              "      <th>...</th>\n",
              "      <th>period_10</th>\n",
              "      <th>period_11</th>\n",
              "      <th>period_12</th>\n",
              "      <th>period_18</th>\n",
              "      <th>period_25</th>\n",
              "      <th>period_32</th>\n",
              "      <th>period_33</th>\n",
              "      <th>period_34</th>\n",
              "      <th>period_35</th>\n",
              "      <th>period_36</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Erik \"\"Ealanrian\"\" Jansman\"</td>\n",
              "      <td>2016-07-11 08:38:37</td>\n",
              "      <td>apache/celix</td>\n",
              "      <td>apache</td>\n",
              "      <td>2016-07-11 08:38:37</td>\n",
              "      <td>2016-11-11 08:38:37</td>\n",
              "      <td>2017-03-11 08:38:37</td>\n",
              "      <td>2017-07-11 08:38:37</td>\n",
              "      <td>2017-11-11 08:38:37</td>\n",
              "      <td>...</td>\n",
              "      <td>2019-11-11 08:38:37</td>\n",
              "      <td>2020-03-11 08:38:37</td>\n",
              "      <td>2017-07-11 08:38:37</td>\n",
              "      <td>2018-01-11 08:38:37</td>\n",
              "      <td>2018-08-11 08:38:37</td>\n",
              "      <td>2019-03-11 08:38:37</td>\n",
              "      <td>2019-04-11 08:38:37</td>\n",
              "      <td>2019-05-11 08:38:37</td>\n",
              "      <td>2019-06-11 08:38:37</td>\n",
              "      <td>2019-07-11 08:38:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>\"James \"\"Chuck\"\" Williams\"</td>\n",
              "      <td>2006-06-08 22:38:37</td>\n",
              "      <td>apache/axis2-java</td>\n",
              "      <td>apache</td>\n",
              "      <td>2006-06-08 22:38:37</td>\n",
              "      <td>2006-10-08 22:38:37</td>\n",
              "      <td>2007-02-08 22:38:37</td>\n",
              "      <td>2007-06-08 22:38:37</td>\n",
              "      <td>2007-10-08 22:38:37</td>\n",
              "      <td>...</td>\n",
              "      <td>2009-10-08 22:38:37</td>\n",
              "      <td>2010-02-08 22:38:37</td>\n",
              "      <td>2007-06-08 22:38:37</td>\n",
              "      <td>2007-12-08 22:38:37</td>\n",
              "      <td>2008-07-08 22:38:37</td>\n",
              "      <td>2009-02-08 22:38:37</td>\n",
              "      <td>2009-03-08 23:38:37</td>\n",
              "      <td>2009-04-08 22:38:37</td>\n",
              "      <td>2009-05-08 22:38:37</td>\n",
              "      <td>2009-06-08 22:38:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                committer_name committer_timestamp         repo_names  \\\n",
              "0   1  \"Erik \"\"Ealanrian\"\" Jansman\" 2016-07-11 08:38:37       apache/celix   \n",
              "1   2    \"James \"\"Chuck\"\" Williams\" 2006-06-08 22:38:37  apache/axis2-java   \n",
              "\n",
              "  repo_org        first_commit            period_1            period_2  \\\n",
              "0   apache 2016-07-11 08:38:37 2016-11-11 08:38:37 2017-03-11 08:38:37   \n",
              "1   apache 2006-06-08 22:38:37 2006-10-08 22:38:37 2007-02-08 22:38:37   \n",
              "\n",
              "             period_3            period_4  ...           period_10  \\\n",
              "0 2017-07-11 08:38:37 2017-11-11 08:38:37  ... 2019-11-11 08:38:37   \n",
              "1 2007-06-08 22:38:37 2007-10-08 22:38:37  ... 2009-10-08 22:38:37   \n",
              "\n",
              "            period_11           period_12           period_18  \\\n",
              "0 2020-03-11 08:38:37 2017-07-11 08:38:37 2018-01-11 08:38:37   \n",
              "1 2010-02-08 22:38:37 2007-06-08 22:38:37 2007-12-08 22:38:37   \n",
              "\n",
              "            period_25           period_32           period_33  \\\n",
              "0 2018-08-11 08:38:37 2019-03-11 08:38:37 2019-04-11 08:38:37   \n",
              "1 2008-07-08 22:38:37 2009-02-08 22:38:37 2009-03-08 23:38:37   \n",
              "\n",
              "            period_34           period_35           period_36  \n",
              "0 2019-05-11 08:38:37 2019-06-11 08:38:37 2019-07-11 08:38:37  \n",
              "1 2009-04-08 22:38:37 2009-05-08 22:38:37 2009-06-08 22:38:37  \n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBjgSujpYRdQ"
      },
      "source": [
        "### 2. P(N)_ numb_commits_org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "GcKxEVGmYRdR"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We take all commits occuring within a specific period of question.\n",
        "2. We perform a count per (i.e. groupby) committer_name and per organization. \n",
        "3. This table represents all commits performed by the developer to the organization within specific time period\n",
        "4. We perform a join of this table to all_table\n",
        "5. This step is repeated for each time period i.e. N,N+1,N+2...N+6\n",
        "6. For rows that are null, we fill with 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBBWDpP4YRdR"
      },
      "source": [
        "## dont change repos to any other name in any of the sub-tables!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Bv6QAT6hYRdR"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P1_numb_commits_org , repo_org as org, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp >= first_commit AND committer_timestamp < period_1\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"P1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DipVXcPcYRdR",
        "outputId": "d1b068b2-c716-4f09-de75-84fa107d833a"
      },
      "source": [
        "#Sample P1 table showing count of commits in first timespan. Notice that the count is grouped by org and committer_name\n",
        "#we will right join this table by org name and com_name to the master table i.e. repos\n",
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM P1\n",
        "LIMIT 2\n",
        "\n",
        "\"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P1_numb_commits_org</th>\n",
              "      <th>org</th>\n",
              "      <th>com_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>apache</td>\n",
              "      <td>Aaron Myers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>apache</td>\n",
              "      <td>Aurélien Pupier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   P1_numb_commits_org     org         com_name\n",
              "0                   43  apache      Aaron Myers\n",
              "1                    1  apache  Aurélien Pupier"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Zy0vZWUSYRdS",
        "outputId": "be07ef17-86ea-432c-987e-437e6b828336"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P1_numb_commits_org , repo_org as org, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp >= first_commit AND committer_timestamp < period_1\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "LIMIT 5\n",
        "\n",
        "\"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P1_numb_commits_org</th>\n",
              "      <th>org</th>\n",
              "      <th>com_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43</td>\n",
              "      <td>apache</td>\n",
              "      <td>Aaron Myers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>apache</td>\n",
              "      <td>Aurélien Pupier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21</td>\n",
              "      <td>apache</td>\n",
              "      <td>Danny Chan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>apache</td>\n",
              "      <td>Evgeny Stanilovskiy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>apache</td>\n",
              "      <td>Fridolin Jackstadt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   P1_numb_commits_org     org             com_name\n",
              "0                   43  apache          Aaron Myers\n",
              "1                    1  apache      Aurélien Pupier\n",
              "2                   21  apache           Danny Chan\n",
              "3                    2  apache  Evgeny Stanilovskiy\n",
              "4                    2  apache   Fridolin Jackstadt"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "E2Z6IySbYRdS"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P2_numb_commits_org , repo_org as org, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"P2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CyAtG9qVYRdS"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P3_numb_commits_org, repo_org as org, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_2 AND committer_timestamp < period_3\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"P3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6kGxZrKHYRdT"
      },
      "source": [
        "t = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P4_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_3 AND committer_timestamp < period_4\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "t.createOrReplaceTempView(\"P4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RgNOPdzhYRdT"
      },
      "source": [
        "r = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P5_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_4 AND committer_timestamp < period_5\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "r.createOrReplaceTempView(\"P5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "f5Q7kd1dYRdT"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P6_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_5 AND committer_timestamp < period_6\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"P6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SsRxrlveYRdT"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P7_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_6 AND committer_timestamp < period_7\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"P7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yvh3z2b4YRdU"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P8_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_7 AND committer_timestamp < period_8\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a6-wm1NGYRdU"
      },
      "source": [
        "e = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P9_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_8 AND committer_timestamp < period_9\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "e.createOrReplaceTempView(\"P9\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a2ubUeMMYRdU"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P10_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_9 AND committer_timestamp < period_10\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "D8D6X2x3YRdU"
      },
      "source": [
        "e = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P11_numb_commits_org, repo_org as org , committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_10 AND committer_timestamp < period_11\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "e.createOrReplaceTempView(\"P11\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "x1nS8yNEYRdU"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as last_commits, repo_org as org, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_12 AND committer_timestamp < period_18\n",
        ")\n",
        "GROUP BY org, com_name\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"last_months\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sD0U6efYRdV"
      },
      "source": [
        "### join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69J3VkSDYRdV"
      },
      "source": [
        "#don't change repos name in first join!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "W7JpdFrLYRdV"
      },
      "source": [
        "# we now join each of the tables one-by-one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JChw1rPsYRdV"
      },
      "source": [
        "#DONT CHANGE content to repos to repos_2 here. Only re-save as repos_2. This is first join.\n",
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,\n",
        "\n",
        "P1_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P1\n",
        ") \n",
        "\n",
        "t RIGHT JOIN repos ON t.org = repos.repo_org AND t.com_name = repos.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"repos_2\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "16_9nY3SYRdW"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P2\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h8tjHlOvYRdW"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P3\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "snZ4r5yfYRdW"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P4\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org  AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iXftxSTfYRdW"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P5\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mWFd00jlYRdX"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit, \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P6\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SKbfYtrGYRdX"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P7\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qwtVIyneYRdX"
      },
      "source": [
        "a = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org, P8_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P8\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "a.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AOpsE0mBYRdX"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org,P8_numb_commits_org,P9_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P9\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CNSZTwEPYRdY"
      },
      "source": [
        "e = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org,P8_numb_commits_org,P9_numb_commits_org,\n",
        "P10_numb_commits_org \n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P10\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "e.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jFC-zkC_YRdY"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org,P8_numb_commits_org,P9_numb_commits_org,\n",
        "P10_numb_commits_org, P11_numb_commits_org\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P11\n",
        ") \n",
        "t RIGHT JOIN repos_2 ON t.org = repos_2.repo_org   AND t.com_name = repos_2.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Kp3J7zIsYRdY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wbntbTWrYRdY"
      },
      "source": [
        "repos_spark = repos_spark.fillna(0, subset=['P1_numb_commits_org', 'P2_numb_commits_org', 'P3_numb_commits_org', \n",
        "                                        'P4_numb_commits_org', 'P5_numb_commits_org', 'P6_numb_commits_org',\n",
        "                                       'P7_numb_commits_org', 'P8_numb_commits_org', 'P9_numb_commits_org',\n",
        "                                       'P10_numb_commits_org', 'P11_numb_commits_org'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DP9Xai7FYRdZ"
      },
      "source": [
        "repos_spark = repos_spark.fillna(0, subset=['last_commits'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "w9YaC3oeYRdZ"
      },
      "source": [
        "repos_spark.createOrReplaceTempView(\"repos_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGI3H19uYRdZ",
        "outputId": "bd877363-7855-491e-f23a-72f3ddf5ff0b"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as count\n",
        "FROM repos_2\n",
        "\n",
        "\"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1048536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     count\n",
              "0  1048536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jqqa4rQYRdZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-A8c57wyYRda"
      },
      "source": [
        "#we notice that some committers probably do \"batch\" commit\n",
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT distinct(committer_name), (P1_numb_commits_org)\n",
        "# FROM repos_2\n",
        "# WHERE P1_numb_commits_org > 80\n",
        "# LIMIT 5\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KbWKihWqYRda"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uKMrR2BSYRda"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_1\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P2_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P3_numb_commits_org)  \n",
        "#     THEN \"disengaged_1\"\n",
        "#     ELSE \"engaged\" END) y_block_1\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_1\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tqGmlk4uYRda"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_2\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P3_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P4_numb_commits_org)  \n",
        "#     THEN \"disengaged_2\"\n",
        "#     ELSE \"engaged\" END) y_block_2\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_2\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e2Wfg9CGYRda"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_3\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P4_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P5_numb_commits_org)  \n",
        "#     THEN \"disengaged_3\"\n",
        "#     ELSE \"engaged\" END) y_block_3\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_3\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2GfoZsUDYRdb"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_4\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P5_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P6_numb_commits_org)  \n",
        "#     THEN \"disengaged_4\"\n",
        "#     ELSE \"engaged\" END) y_block_4\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_4\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "R2OQAOKMYRdb"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_5\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P6_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P7_numb_commits_org)  \n",
        "#     THEN \"disengaged_5\"\n",
        "#     ELSE \"engaged\" END) y_block_5\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_5\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6_auEYi7YRdb"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_6\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P7_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P8_numb_commits_org)  \n",
        "#     THEN \"disengaged_6\"\n",
        "#     ELSE \"engaged\" END) y_block_6\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_6\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPSlIPMrYRdb"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_7\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P8_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P9_numb_commits_org)  \n",
        "#     THEN \"disengaged_7\"\n",
        "#     ELSE \"engaged\" END) y_block_7\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_7\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PkGn5lwyYRdc"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count, y_block_8\n",
        "# FROM\n",
        "# (\n",
        "# SELECT \n",
        "#     (CASE WHEN ((P1_numb_commits_org * 0.2) > P9_numb_commits_org \n",
        "#     AND (P1_numb_commits_org * 0.2) > P10_numb_commits_org)  \n",
        "#     THEN \"disengaged_8\"\n",
        "#     ELSE \"engaged\" END) y_block_8\n",
        "    \n",
        "# FROM repos_2\n",
        "# )\n",
        "# GROUP BY y_block_8\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne44rjwEYRdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIkHWgPYYRdc"
      },
      "source": [
        "### 3. P(N)_ numb_commits_repo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "C6wiHEHIYRdd"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We take all commits occuring within a specific period of question.\n",
        "2. We perform a count per (i.e. groupby) committer_name and per repo. \n",
        "3. This table represents all commits performed by the developer to the repo within specific time period\n",
        "4. We perform a join of this table to all_table\n",
        "5. This step is repeated for each time period i.e. N,N+1,N+2...N+6. For V1, I only do till P2.\n",
        "6. For rows that are null (i.e. no commits by developer during time period) we fill with 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os2jDnmtYRdd"
      },
      "source": [
        "### make table for each successive month duration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6M9sHXK_YRdd"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P1_numb_commits_repo , repo_names as rep_names, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp >= first_commit AND committer_timestamp < period_1\n",
        ")\n",
        "GROUP BY rep_names, com_name\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P1_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PnFBsNYSYRdd"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P2_numb_commits_repo , repo_names as rep_names, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY rep_names, com_name\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"P2_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EUrANbUuYRde"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P3_numb_commits_repo , repo_names as rep_names, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY rep_names, com_name\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"P3_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vSRjmApYYRde"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P4_numb_commits_repo , repo_names as rep_names, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY rep_names, com_name\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"P4_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "byUuUnrOYRde"
      },
      "source": [
        "d = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P5_numb_commits_repo , repo_names as rep_names, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY rep_names, com_name\n",
        "\n",
        "\"\"\")\n",
        "d.createOrReplaceTempView(\"P5_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zjKME_UuYRdf"
      },
      "source": [
        "i = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as P6_numb_commits_repo , repo_names as rep_names, committer_name as com_name\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY rep_names, com_name\n",
        "\n",
        "\"\"\")\n",
        "i.createOrReplaceTempView(\"P6_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "S6JZTJNIYRdf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzEDFiXPYRdf"
      },
      "source": [
        "### join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reO6Z-VyYRdf"
      },
      "source": [
        "#don't change repos name in first join."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pPFZX-uLYRdf"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P1_repo\n",
        ") \n",
        "t RIGHT JOIN repos ON t.rep_names = repos.repo_names AND t.com_name = repos.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bZfSDdo4YRdg"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_3.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo, P2_numb_commits_repo\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P2_repo\n",
        ") \n",
        "t RIGHT JOIN repos_3 ON t.rep_names = repos_3.repo_names  AND t.com_name = repos_3.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"repos_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c6N35Q3gYRdg"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT *\n",
        "# FROM repos_3\n",
        "# ORDER BY P1_numb_commits_org ASC\n",
        "# LIMIT 2\n",
        "\n",
        "\n",
        "# \"\"\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CKz2vTEHYRdg"
      },
      "source": [
        "a = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_3.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo, P2_numb_commits_repo, P3_numb_commits_repo\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P3_repo\n",
        ") \n",
        "t RIGHT JOIN repos_3 ON t.rep_names = repos_3.repo_names  AND t.com_name = repos_3.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "a.createOrReplaceTempView(\"repos_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_wqtlT2CYRdg"
      },
      "source": [
        "r = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_3.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo, P2_numb_commits_repo, P3_numb_commits_repo,\n",
        "P4_numb_commits_repo\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P4_repo\n",
        ") \n",
        "t RIGHT JOIN repos_3 ON t.rep_names = repos_3.repo_names  AND t.com_name = repos_3.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "r.createOrReplaceTempView(\"repos_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SZZO-16SYRdh"
      },
      "source": [
        "y = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_3.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo, P2_numb_commits_repo, P3_numb_commits_repo,\n",
        "P4_numb_commits_repo, P5_numb_commits_repo\n",
        "\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P5_repo\n",
        ") \n",
        "t RIGHT JOIN repos_3 ON t.rep_names = repos_3.repo_names  AND t.com_name = repos_3.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "y.createOrReplaceTempView(\"repos_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yuL_kI43YRdh"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_3.ID ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo, P2_numb_commits_repo,P3_numb_commits_repo,\n",
        "P4_numb_commits_repo, P5_numb_commits_repo, P6_numb_commits_repo\n",
        "\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P6_repo\n",
        ") \n",
        "t RIGHT JOIN repos_3 ON t.rep_names = repos_3.repo_names  AND t.com_name = repos_3.committer_name\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xwp4IJ8FYRdh"
      },
      "source": [
        "repos_spark = repos_spark.fillna(0, subset=['P1_numb_commits_repo', 'P2_numb_commits_repo',\n",
        "                                       'P3_numb_commits_repo', 'P4_numb_commits_repo',\n",
        "                                       'P5_numb_commits_repo', 'P6_numb_commits_repo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lr8k24N3YRdh"
      },
      "source": [
        "repos_spark.createOrReplaceTempView(\"repos_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvxYLEg8YRdi",
        "outputId": "85c7c56f-0588-4c34-a61e-605223ebfd64"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(1) as count\n",
        "FROM repos_3\n",
        "\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|  count|\n",
            "+-------+\n",
            "|1048536|\n",
            "+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUTSDs2YRdi"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count\n",
        "# FROM repos_3\n",
        "\n",
        "# \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjixvSCdYRdi"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT count(1) as count\n",
        "# FROM repos_join\n",
        "\n",
        "# \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haHHLxD7YRdi",
        "outputId": "ee60423b-7579-4b4d-bb24-619987e89584"
      },
      "source": [
        "repos_spark.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'P1_numb_commits_org',\n",
              " 'P2_numb_commits_org',\n",
              " 'P3_numb_commits_org',\n",
              " 'P4_numb_commits_org',\n",
              " 'P5_numb_commits_org',\n",
              " 'P6_numb_commits_org',\n",
              " 'P7_numb_commits_org',\n",
              " 'P8_numb_commits_org',\n",
              " 'P9_numb_commits_org',\n",
              " 'P10_numb_commits_org',\n",
              " 'P11_numb_commits_org',\n",
              " 'P1_numb_commits_repo',\n",
              " 'P2_numb_commits_repo',\n",
              " 'P3_numb_commits_repo',\n",
              " 'P4_numb_commits_repo',\n",
              " 'P5_numb_commits_repo',\n",
              " 'P6_numb_commits_repo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjlyyqtYYRdj"
      },
      "source": [
        "## 4. P(N)_ repo_comm_absent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "mLhdEqT-YRdj"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We take all commits occuring within a specific period of question (e.g. first).\n",
        "2. We take all commits occuring in the successive period of Pt. 1 (e.g. second). \n",
        "3. We take a count of all cases where a committer name appears in first but not in second and store in a temptable\n",
        "4. We perform a join of this table to all_table\n",
        "5. This step is repeated for each time period i.e. N,N+1,N+2...N+6. For V1, I only do first two periods.\n",
        "6. For rows that are null (i.e. no commits by developer during time period) we fill with 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qYdsx97qYRdj"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT committer_name, repo_names \n",
        "FROM repos\n",
        "WHERE committer_timestamp >= first_commit AND committer_timestamp < period_1\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P_at_1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aFtuHLHXYRdk"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT committer_name, repo_names \n",
        "FROM repos\n",
        "WHERE committer_timestamp >= period_1 AND committer_timestamp < period_2\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"P_at_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axuYUnWSYRdk"
      },
      "source": [
        "a = spark.sql(\"\"\"\n",
        "\n",
        "SELECT committer_name, repo_names \n",
        "FROM repos\n",
        "WHERE committer_timestamp >= period_2 AND committer_timestamp < period_3\n",
        "\n",
        "\"\"\")\n",
        "a.createOrReplaceTempView(\"P_at_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKkWl2WYRdk"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT committer_name, repo_names \n",
        "FROM repos\n",
        "WHERE committer_timestamp >= period_3 AND committer_timestamp < period_4\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"P_at_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir9aQ7fxYRdk"
      },
      "source": [
        "t = spark.sql(\"\"\"\n",
        "\n",
        "SELECT committer_name, repo_names \n",
        "FROM repos\n",
        "WHERE committer_timestamp >= period_4 AND committer_timestamp < period_5\n",
        "\n",
        "\"\"\")\n",
        "t.createOrReplaceTempView(\"P_at_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORuRxHgXYRdl"
      },
      "source": [
        "e = spark.sql(\"\"\"\n",
        "\n",
        "SELECT committer_name, repo_names \n",
        "FROM repos\n",
        "WHERE committer_timestamp >= period_5 AND committer_timestamp < period_6\n",
        "\n",
        "\"\"\")\n",
        "e.createOrReplaceTempView(\"P_at_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpKVyFN_YRdl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXnEfpvLYRdl"
      },
      "source": [
        "### make temp tables that compares tables for each successive period"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "T_B0PF47YRdl"
      },
      "source": [
        "x = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P1_repo_comm_absent, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t1.committer_name as comm_name, t1.repo_names as rep_name\n",
        "FROM P_at_1 t1\n",
        "WHERE NOT EXISTS (SELECT t2.committer_name FROM P_at_2 t2 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "x.createOrReplaceTempView(\"P_2_1_repo_comm_absent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OopVynqfYRdm"
      },
      "source": [
        "e = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P2_repo_comm_absent, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t1.committer_name as comm_name, t1.repo_names as rep_name\n",
        "FROM P_at_2 t1\n",
        "WHERE NOT EXISTS (SELECT t2.committer_name FROM P_at_3 t2 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "e.createOrReplaceTempView(\"P_3_2_repo_comm_absent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK8FME7YYRdm"
      },
      "source": [
        "a = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P3_repo_comm_absent, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t1.committer_name as comm_name, t1.repo_names as rep_name\n",
        "FROM P_at_3 t1\n",
        "WHERE NOT EXISTS (SELECT t2.committer_name FROM P_at_4 t2 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "a.createOrReplaceTempView(\"P_4_3_repo_comm_absent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCWGLr6uYRdm"
      },
      "source": [
        "w = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P4_repo_comm_absent, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t1.committer_name as comm_name, t1.repo_names as rep_name\n",
        "FROM P_at_4 t1\n",
        "WHERE NOT EXISTS (SELECT t2.committer_name FROM P_at_5 t2 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "w.createOrReplaceTempView(\"P_5_4_repo_comm_absent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ewcAWjYRdm"
      },
      "source": [
        "r = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P5_repo_comm_absent, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t1.committer_name as comm_name, t1.repo_names as rep_name\n",
        "FROM P_at_5 t1\n",
        "WHERE NOT EXISTS (SELECT t2.committer_name FROM P_at_6 t2 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "r.createOrReplaceTempView(\"P_6_5_repo_comm_absent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhbs4P_HYRdm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBTojFnpYRdn"
      },
      "source": [
        "### joining to repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bVXaHNIYRdn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4815IA6IYRdn"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_absent\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_2_1_repo_comm_absent\n",
        ") \n",
        "t RIGHT JOIN repos ON t.rep_name = repos.repo_names \n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"repos_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTENjnIJYRdn"
      },
      "source": [
        "repos_4_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_4.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_absent,P2_repo_comm_absent\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_3_2_repo_comm_absent\n",
        ") \n",
        "t RIGHT JOIN repos_4 ON t.rep_name = repos_4.repo_names \n",
        "\n",
        "\"\"\")\n",
        "\n",
        "repos_4_spark.createOrReplaceTempView(\"repos_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_EnM-ajYRdo"
      },
      "source": [
        "a = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_4.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_absent,P2_repo_comm_absent,P3_repo_comm_absent\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_4_3_repo_comm_absent\n",
        ") \n",
        "t RIGHT JOIN repos_4 ON t.rep_name = repos_4.repo_names \n",
        "\n",
        "\"\"\")\n",
        "a.createOrReplaceTempView(\"repos_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmZ_Wzg6YRdo"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_4.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_absent,P2_repo_comm_absent,P3_repo_comm_absent,\n",
        "P4_repo_comm_absent\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_5_4_repo_comm_absent\n",
        ") \n",
        "t RIGHT JOIN repos_4 ON t.rep_name = repos_4.repo_names \n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"repos_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZL7IgIqYRdo"
      },
      "source": [
        "repos_4_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_4.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "\n",
        "P1_repo_comm_absent,P2_repo_comm_absent,P3_repo_comm_absent,\n",
        "P4_repo_comm_absent,P5_repo_comm_absent\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_6_5_repo_comm_absent\n",
        ") \n",
        "t RIGHT JOIN repos_4 ON t.rep_name = repos_4.repo_names \n",
        "\n",
        "\"\"\")\n",
        "repos_4_spark.createOrReplaceTempView(\"repos_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "m9NOjCBiYRdo"
      },
      "source": [
        "repos_4_spark = repos_4_spark.fillna(0, subset=['P1_repo_comm_absent','P2_repo_comm_absent',\n",
        "                                        'P3_repo_comm_absent','P4_repo_comm_absent',\n",
        "                                        'P5_repo_comm_absent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qheyVl0PYRdp"
      },
      "source": [
        "repos_4_spark.createOrReplaceTempView(\"repos_4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzIrtWQgYRdp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmxCcetYYRdp"
      },
      "source": [
        "### 5. P(N)_ repo_comm_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "EWUYOhu6YRdp"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We take all commits occuring within a specific period of question (e.g. first).\n",
        "2. We take all commits occuring in the successive period of Pt. 1 (e.g. second). \n",
        "3. We take a count of all cases where a committer name appears in second but not in first, and store in a temptable\n",
        "4. We perform a join of this table to all_table\n",
        "5. This step is repeated for each time period i.e. N,N+1,N+2...N+6. For V1, I only do first two periods.\n",
        "6. For rows that are null (i.e. no commits by developer during time period) we fill with 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NQZQonC0YRdp"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P1_repo_comm_new, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t2.committer_name as comm_name, t2.repo_names as rep_name\n",
        "FROM P_at_2 t2\n",
        "WHERE NOT EXISTS (SELECT t1.committer_name FROM P_at_1 t1 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"P_2_1_repo_comm_new\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX-C000IYRdq"
      },
      "source": [
        "q = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P2_repo_comm_new, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t2.committer_name as comm_name, t2.repo_names as rep_name\n",
        "FROM P_at_3 t2\n",
        "WHERE NOT EXISTS (SELECT t1.committer_name FROM P_at_2 t1 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "q.createOrReplaceTempView(\"P_3_2_repo_comm_new\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unBxtgxmYRdq"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P3_repo_comm_new, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t2.committer_name as comm_name, t2.repo_names as rep_name\n",
        "FROM P_at_4 t2\n",
        "WHERE NOT EXISTS (SELECT t1.committer_name FROM P_at_3 t1 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P_4_3_repo_comm_new\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pOdy7YnYRdq"
      },
      "source": [
        "r = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P4_repo_comm_new, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t2.committer_name as comm_name, t2.repo_names as rep_name\n",
        "FROM P_at_5 t2\n",
        "WHERE NOT EXISTS (SELECT t1.committer_name FROM P_at_4 t1 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "r.createOrReplaceTempView(\"P_5_4_repo_comm_new\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN-68XW-YRdq"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P5_repo_comm_new, rep_name\n",
        "FROM\n",
        "(\n",
        "SELECT t2.committer_name as comm_name, t2.repo_names as rep_name\n",
        "FROM P_at_6 t2\n",
        "WHERE NOT EXISTS (SELECT t1.committer_name FROM P_at_5 t1 WHERE t1.committer_name = t2.committer_name AND \n",
        "                    t1.repo_names = t2.repo_names)\n",
        ")\n",
        "GROUP BY rep_name\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"P_6_5_repo_comm_new\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ml-0aaSYRdr"
      },
      "source": [
        "### joining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yBfjOfPwYRdr"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_new\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_2_1_repo_comm_new\n",
        ") \n",
        "t RIGHT JOIN repos ON t.rep_name = repos.repo_names \n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7GS1Or3YRdr"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_5.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_new, P2_repo_comm_new\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_3_2_repo_comm_new\n",
        ") \n",
        "t RIGHT JOIN repos_5 ON t.rep_name = repos_5.repo_names \n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"repos_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjQLHusNYRdr"
      },
      "source": [
        "g = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_5.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_new, P2_repo_comm_new, P3_repo_comm_new\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_4_3_repo_comm_new\n",
        ") \n",
        "t RIGHT JOIN repos_5 ON t.rep_name = repos_5.repo_names \n",
        "\n",
        "\"\"\")\n",
        "g.createOrReplaceTempView(\"repos_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw_H3pj0YRds"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_5.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit, \n",
        "\n",
        "P1_repo_comm_new, P2_repo_comm_new, P3_repo_comm_new, \n",
        "P4_repo_comm_new\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_5_4_repo_comm_new\n",
        ") \n",
        "t RIGHT JOIN repos_5 ON t.rep_name = repos_5.repo_names \n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESnA7qo7YRds"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_5.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_comm_new, P2_repo_comm_new, P3_repo_comm_new, \n",
        "P4_repo_comm_new, P5_repo_comm_new\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P_6_5_repo_comm_new\n",
        ") \n",
        "t RIGHT JOIN repos_5 ON t.rep_name = repos_5.repo_names \n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Zh49QmVXYRds"
      },
      "source": [
        "all_table = all_table.fillna(0, subset=['P1_repo_comm_new', 'P2_repo_comm_new', \n",
        "                                        'P3_repo_comm_new', 'P4_repo_comm_new',\n",
        "                                        'P5_repo_comm_new'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Rx6c2rivYRds"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D0FaZpUYRdt"
      },
      "source": [
        "## 6. P(N)_ repo_committers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "8cRlkiytYRdt"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We take all commits occuring within a specific period of question.\n",
        "2. We perform a count per (i.e. groupby) distinct committer_name and repo\n",
        "3. This table represents the count of unique developers who have committed to the repos within specific time period\n",
        "4. We perform a join of this table to all_table\n",
        "5. This step is repeated for each time period i.e. N,N+1,N+2...N+6. For V1, I only do till P2.\n",
        "6. For rows that are null (i.e. no commits by developer during time period) we fill with 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hEERSmelYRdt"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(comm_name)) as P1_repo_committers, rep_names\n",
        "FROM\n",
        "(\n",
        "SELECT committer_name as comm_name, repo_names as rep_names\n",
        "FROM repos\n",
        "WHERE committer_timestamp >= first_commit AND committer_timestamp < period_1\n",
        ")\n",
        "GROUP BY rep_names\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"P1_person_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2iIBrCv1YRdt"
      },
      "source": [
        "a = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(committer_name)) as P2_repo_committers, repo_names as rep_names\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_1 AND committer_timestamp < period_2\n",
        ")\n",
        "GROUP BY rep_names\n",
        "\n",
        "\"\"\")\n",
        "a.createOrReplaceTempView(\"P2_person_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2xRVAfMYRdu"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(committer_name)) as P3_repo_committers, repo_names as rep_names\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_2 AND committer_timestamp < period_3\n",
        ")\n",
        "GROUP BY rep_names\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P3_person_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEKziwWaYRdu"
      },
      "source": [
        "f = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(committer_name)) as P4_repo_committers, repo_names as rep_names\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_3 AND committer_timestamp < period_4\n",
        ")\n",
        "GROUP BY rep_names\n",
        "\n",
        "\"\"\")\n",
        "f.createOrReplaceTempView(\"P4_person_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrKH0gy7YRdv"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(committer_name)) as P5_repo_committers, repo_names as rep_names\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_4 AND committer_timestamp < period_5\n",
        ")\n",
        "GROUP BY rep_names\n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"P5_person_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znNoLXNDYRdv"
      },
      "source": [
        "g = spark.sql(\"\"\"\n",
        "\n",
        "SELECT count(distinct(committer_name)) as P6_repo_committers, repo_names as rep_names\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM repos\n",
        "WHERE committer_timestamp > period_5 AND committer_timestamp < period_6\n",
        ")\n",
        "GROUP BY rep_names\n",
        "\n",
        "\"\"\")\n",
        "g.createOrReplaceTempView(\"P6_person_table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6QSqo-YRdv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U2Qu_yPYRdv"
      },
      "source": [
        "### Joining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Hgl3k_v2YRdv"
      },
      "source": [
        "g = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_committers\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P1_person_table\n",
        ") \n",
        "t RIGHT JOIN repos ON t.rep_names = repos.repo_names \n",
        "\n",
        "\"\"\")\n",
        "g.createOrReplaceTempView(\"repos_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EqBNay8YRdw"
      },
      "source": [
        "g = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_6.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_committers, P2_repo_committers\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P2_person_table\n",
        ") \n",
        "t RIGHT JOIN repos_6 ON t.rep_names = repos_6.repo_names \n",
        "\n",
        "\"\"\")\n",
        "g.createOrReplaceTempView(\"repos_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nqEF34sNYRdw"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_6.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,\n",
        "\n",
        "P1_repo_committers, P2_repo_committers, P3_repo_committers\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P3_person_table\n",
        ") \n",
        "t RIGHT JOIN repos_6 ON t.rep_names = repos_6.repo_names \n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3f0GjfuYRdw"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_6.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_committers, P2_repo_committers, P3_repo_committers, \n",
        "P4_repo_committers\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P4_person_table\n",
        ") \n",
        "t RIGHT JOIN repos_6 ON t.rep_names = repos_6.repo_names \n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCcN_bPaYRdw"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_6.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_committers, P2_repo_committers, P3_repo_committers, \n",
        "P4_repo_committers, P5_repo_committers\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P5_person_table\n",
        ") \n",
        "t RIGHT JOIN repos_6 ON t.rep_names = repos_6.repo_names \n",
        "\n",
        "\"\"\")\n",
        "s.createOrReplaceTempView(\"repos_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjnC0ZC5YRdx"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_6.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_repo_committers, P2_repo_committers, P3_repo_committers, \n",
        "P4_repo_committers, P5_repo_committers, P6_repo_committers\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT *\n",
        "FROM P6_person_table\n",
        ") \n",
        "t RIGHT JOIN repos_6 ON t.rep_names = repos_6.repo_names \n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3UeHxVrsYRdx"
      },
      "source": [
        "all_table = all_table.fillna(0, subset=['P1_repo_committers', 'P2_repo_committers', \n",
        "                                        'P3_repo_committers', 'P4_repo_committers', \n",
        "                                        'P5_repo_committers', 'P6_repo_committers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GPDIoVfGYRdx"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP89-CgzYRdx"
      },
      "source": [
        "## 7. total_repo_comm_absent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "xX4BgfYdYRdy"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We add up columns P1_repo_comm_absent (N=1 till N=6) to the all_table\n",
        "2. For rows that are null (i.e. no commits by developer during time period) we fill with 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5VWNKoWYRdz"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_4.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "(P1_repo_comm_absent+P2_repo_comm_absent+P3_repo_comm_absent+P4_repo_comm_absent+P5_repo_comm_absent) tot_repo_comm_absent\n",
        "\n",
        "FROM repos_4\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G37izqSOYRdz"
      },
      "source": [
        "all_table = all_table.fillna(0, subset=['tot_repo_comm_absent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nr2Ug54YRd0"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K475RN7_YRd0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrwLRVjNYRd1"
      },
      "source": [
        "## 8. total_repo_comm_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "cd9IbgMPYRd1"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. We add up columns P1_repo_comm_new (N=1 till N=6) to the all_table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLkx5MprYRd1"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_5.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "(P1_repo_comm_new+ P2_repo_comm_new+P3_repo_comm_new+ P4_repo_comm_new+P5_repo_comm_new) total_repo_comm_new\n",
        "\n",
        "FROM repos_5\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZZQXcx8YRd2"
      },
      "source": [
        "all_table = all_table.fillna(0, subset=['total_repo_comm_new'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwag7oueYRd2"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mANINhb6YRd2"
      },
      "source": [
        "## 9. agg_absent_periods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "OdwWdBjeYRd2"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. Use a for loop within the F.when syntax stating the condition. The statement also adds a column to all_table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHp32lN4YRd2"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM repos_4\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0jgjLRhjYRd3",
        "outputId": "31980f63-bd81-4c58-cb9a-1ed420b95606"
      },
      "source": [
        "# to edit slice integer depending on position of concerning columns\n",
        "all_table.columns[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['P1_repo_comm_absent',\n",
              " 'P2_repo_comm_absent',\n",
              " 'P3_repo_comm_absent',\n",
              " 'P4_repo_comm_absent',\n",
              " 'P5_repo_comm_absent']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sWaFL5m3YRd3"
      },
      "source": [
        "all_table = all_table.withColumn(\"agg_absent_periods\",sum([F.when(F.col(cl) != 0, 1).otherwise(0) for cl in all_table.columns[-5:]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1u3kragYRd3",
        "outputId": "ba4bac94-6d37-41f4-ea6c-599adac8a0a8"
      },
      "source": [
        "all_table.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'P1_repo_comm_absent',\n",
              " 'P2_repo_comm_absent',\n",
              " 'P3_repo_comm_absent',\n",
              " 'P4_repo_comm_absent',\n",
              " 'P5_repo_comm_absent',\n",
              " 'agg_absent_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UvhjEuPYRd4"
      },
      "source": [
        "all_table_1 = all_table.select([c for c in all_table.columns if c not in \n",
        "                                    {'P1_repo_comm_absent',\n",
        " 'P2_repo_comm_absent',\n",
        " 'P3_repo_comm_absent',\n",
        " 'P4_repo_comm_absent',\n",
        " 'P5_repo_comm_absent'}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvJbxSeFYRd5",
        "outputId": "39ded473-b3d2-42e1-c7e7-3f9531686f2c"
      },
      "source": [
        "all_table_1.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'agg_absent_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J02lvq8hYRd5"
      },
      "source": [
        "all_table_1.registerTempTable(\"repos_9\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAjwe_baYRd5"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT AVG(agg_absent_periods), MIN(agg_absent_periods), MAX(agg_absent_periods)\n",
        "# FROM repos_9\n",
        "\n",
        "# \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbPhxGpoYRd5"
      },
      "source": [
        "## 10. new_ppl_periods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "ja3GRy7rYRd6"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "\n",
        "1. Use a for loop within the F.when syntax stating the condition. The statement also adds a column to all_table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22fvB_7BYRd6"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM repos_5\n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_4d6_f01YRd6",
        "outputId": "9ccd5ae7-ed9f-43f4-aec5-4617ba02a880"
      },
      "source": [
        "# to edit slice integer depending on concerning col position\n",
        "all_table.columns[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['P1_repo_comm_new',\n",
              " 'P2_repo_comm_new',\n",
              " 'P3_repo_comm_new',\n",
              " 'P4_repo_comm_new',\n",
              " 'P5_repo_comm_new']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5GAEUO8CYRd7"
      },
      "source": [
        "all_table = all_table.withColumn(\"new_ppl_periods\",sum([F.when(F.col(cl) != 0, 1).otherwise(0) for cl in all_table.columns[-5:]]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BPBP2rPYRd7",
        "outputId": "6c0a767b-3881-43dc-8fd2-0bd45e80d3b1"
      },
      "source": [
        "all_table.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'P1_repo_comm_new',\n",
              " 'P2_repo_comm_new',\n",
              " 'P3_repo_comm_new',\n",
              " 'P4_repo_comm_new',\n",
              " 'P5_repo_comm_new',\n",
              " 'new_ppl_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Nezw9AYRd7"
      },
      "source": [
        "all_table_1 = all_table.select([c for c in all_table.columns if c not in \n",
        "                                    {'P1_repo_comm_new',\n",
        " 'P2_repo_comm_new',\n",
        " 'P3_repo_comm_new',\n",
        " 'P4_repo_comm_new',\n",
        " 'P5_repo_comm_new'}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcj7sM4EYRd7",
        "outputId": "5666835e-1089-4a54-8f31-60413348f668"
      },
      "source": [
        "all_table_1.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'new_ppl_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBLrcRPFYRd8"
      },
      "source": [
        "all_table_1.registerTempTable(\"repos_10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN1foXtbYRd8"
      },
      "source": [
        "# spark.sql(\"\"\"\n",
        "\n",
        "# SELECT AVG(new_ppl_periods), MIN(new_ppl_periods), MAX(new_ppl_periods)\n",
        "# FROM repos_10\n",
        "\n",
        "# \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NrAwRY_-YRd8"
      },
      "source": [
        "# all_table = all_table.fillna(0, subset=['P1_repo_committers', 'P2_repo_committers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l-XAXmPVYRd8"
      },
      "source": [
        "# all_table.createOrReplaceTempView(\"repos\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfAqHtDGYRd8"
      },
      "source": [
        "## 11. no_change_periods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "lEO_8XkLYRd9"
      },
      "source": [
        "The following steps are taken to create this feature:\n",
        "\n",
        "1. Use a for loop within the F.when syntax stating the condition. The statement also adds a column to all_table\n",
        "2. For rows that are null, we fill with 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptD7lWTaYRd9"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT ID, committer_name, committer_timestamp, \n",
        "repo_names, repo_org,first_commit, \n",
        "\n",
        "P1_repo_comm_absent, P2_repo_comm_absent, P3_repo_comm_absent, \n",
        "P4_repo_comm_absent, P5_repo_comm_absent,\n",
        "\n",
        "\n",
        "P1_repo_comm_new, P2_repo_comm_new,P3_repo_comm_new,\n",
        "P4_repo_comm_new, P5_repo_comm_new\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT ID as t_ID , committer_name as t_committer_name, committer_timestamp as t_committer_name, \n",
        "repo_names as  t_repo_names, repo_org  as t_repo_org, first_commit as t_first_commit,\n",
        "\n",
        "P1_repo_comm_absent, P2_repo_comm_absent, P3_repo_comm_absent, \n",
        "P4_repo_comm_absent, P5_repo_comm_absent\n",
        "\n",
        "FROM repos_4\n",
        ") \n",
        "\n",
        "t JOIN repos_5 ON t.t_ID = repos_5.ID\n",
        "    \n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "all_table.createOrReplaceTempView(\"repos_4_5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yE60hG3YReA",
        "outputId": "e932adf1-f9b5-4502-981f-18214e040c29"
      },
      "source": [
        "all_table.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'P1_repo_comm_absent',\n",
              " 'P2_repo_comm_absent',\n",
              " 'P3_repo_comm_absent',\n",
              " 'P4_repo_comm_absent',\n",
              " 'P5_repo_comm_absent',\n",
              " 'P1_repo_comm_new',\n",
              " 'P2_repo_comm_new',\n",
              " 'P3_repo_comm_new',\n",
              " 'P4_repo_comm_new',\n",
              " 'P5_repo_comm_new']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gPkzBP8HYReA"
      },
      "source": [
        "# to extend this code for more periods\n",
        "\n",
        "all_table = all_table.withColumn(\"P1_no_change_periods\",\n",
        "                F.when((all_table[\"P1_repo_comm_absent\"] != 0) & (all_table[\"P1_repo_comm_new\"] != 0), 1).otherwise(0))\n",
        "\n",
        "all_table = all_table.withColumn(\"P2_no_change_periods\",\n",
        "                F.when((all_table[\"P2_repo_comm_absent\"] != 0) & (all_table[\"P2_repo_comm_new\"] != 0), 1).otherwise(0))  \n",
        "\n",
        "\n",
        "all_table = all_table.withColumn(\"P3_no_change_periods\",\n",
        "                F.when((all_table[\"P3_repo_comm_absent\"] != 0) & (all_table[\"P3_repo_comm_new\"] != 0), 1).otherwise(0))  \n",
        "\n",
        "\n",
        "all_table = all_table.withColumn(\"P4_no_change_periods\",\n",
        "                F.when((all_table[\"P4_repo_comm_absent\"] != 0) & (all_table[\"P4_repo_comm_new\"] != 0), 1).otherwise(0))  \n",
        "\n",
        "all_table = all_table.withColumn(\"P5_no_change_periods\",\n",
        "                F.when((all_table[\"P5_repo_comm_absent\"] != 0) & (all_table[\"P5_repo_comm_new\"] != 0), 1).otherwise(0))  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zffhafM8YReB",
        "outputId": "0bcfb104-784d-4878-a700-2ef5372e3957"
      },
      "source": [
        "# to edit slice integer depending on position of concerning columns\n",
        "all_table.columns[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['P1_no_change_periods',\n",
              " 'P2_no_change_periods',\n",
              " 'P3_no_change_periods',\n",
              " 'P4_no_change_periods',\n",
              " 'P5_no_change_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SzEMPFkfYReB"
      },
      "source": [
        "# change slice integer to reflect the cols P(N)_no_change_periods\n",
        "all_table = all_table.withColumn(\"no_change_periods\",\n",
        "sum([F.when(F.col(cl) != 0, 1).otherwise(0) for cl in all_table.columns[-5:]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbWdqb9_YReB",
        "outputId": "e5b20c24-9109-420b-829f-ef8deb223fa8"
      },
      "source": [
        "all_table.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'P1_repo_comm_absent',\n",
              " 'P2_repo_comm_absent',\n",
              " 'P3_repo_comm_absent',\n",
              " 'P4_repo_comm_absent',\n",
              " 'P5_repo_comm_absent',\n",
              " 'P1_repo_comm_new',\n",
              " 'P2_repo_comm_new',\n",
              " 'P3_repo_comm_new',\n",
              " 'P4_repo_comm_new',\n",
              " 'P5_repo_comm_new',\n",
              " 'P1_no_change_periods',\n",
              " 'P2_no_change_periods',\n",
              " 'P3_no_change_periods',\n",
              " 'P4_no_change_periods',\n",
              " 'P5_no_change_periods',\n",
              " 'no_change_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qYIdG4PYReC"
      },
      "source": [
        "all_table_1 = all_table.select([c for c in all_table.columns if c not in \n",
        "                                    {'P1_repo_comm_absent',\n",
        " 'P2_repo_comm_absent',\n",
        " 'P3_repo_comm_absent',\n",
        " 'P4_repo_comm_absent',\n",
        " 'P5_repo_comm_absent',\n",
        " 'P1_repo_comm_new',\n",
        " 'P2_repo_comm_new',\n",
        " 'P3_repo_comm_new',\n",
        " 'P4_repo_comm_new',\n",
        " 'P5_repo_comm_new'}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m6Q8JeiYReC",
        "outputId": "e0239209-71cc-40dd-9f4e-e9d5be70124b"
      },
      "source": [
        "all_table_1.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ID',\n",
              " 'committer_name',\n",
              " 'committer_timestamp',\n",
              " 'repo_names',\n",
              " 'repo_org',\n",
              " 'first_commit',\n",
              " 'P1_no_change_periods',\n",
              " 'P2_no_change_periods',\n",
              " 'P3_no_change_periods',\n",
              " 'P4_no_change_periods',\n",
              " 'P5_no_change_periods',\n",
              " 'no_change_periods']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O-XzBxwNYReC"
      },
      "source": [
        "# all_table = all_table.fillna(0, subset=['P1_repo_committers', 'P2_repo_committers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "evQj3oqhYReD"
      },
      "source": [
        "all_table_1.createOrReplaceTempView(\"repos_11\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVsu_MK0YReD"
      },
      "source": [
        "## 12. P(n)_ multi_ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUpIuNvmYReD"
      },
      "source": [
        "repos_spark = spark.sql(\"\"\"\n",
        "\n",
        "SELECT ID, committer_name, committer_timestamp, \n",
        "repo_names, repo_org,first_commit, \n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org,P8_numb_commits_org,P9_numb_commits_org,\n",
        "P10_numb_commits_org, P11_numb_commits_org,\n",
        "\n",
        "\n",
        "P1_numb_commits_repo, P2_numb_commits_repo,P3_numb_commits_repo,\n",
        "P4_numb_commits_repo, P5_numb_commits_repo, P6_numb_commits_repo\n",
        "\n",
        "FROM\n",
        "(\n",
        "SELECT ID as t_ID , committer_name as t_committer_name, committer_timestamp as t_committer_name, \n",
        "repo_names as  t_repo_names, repo_org  as t_repo_org, first_commit as t_first_commit,\n",
        "\n",
        "P1_numb_commits_org, P2_numb_commits_org, P3_numb_commits_org, \n",
        "P4_numb_commits_org, P5_numb_commits_org,P6_numb_commits_org, \n",
        "P7_numb_commits_org,P8_numb_commits_org,P9_numb_commits_org,\n",
        "P10_numb_commits_org, P11_numb_commits_org\n",
        "\n",
        "FROM repos_2\n",
        ") \n",
        "\n",
        "t JOIN repos_3 ON t.t_ID = repos_3.ID\n",
        "    \n",
        "ORDER BY ID ASC\n",
        "\n",
        "\"\"\")\n",
        "repos_spark.createOrReplaceTempView(\"repos_org_repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "m1wh031NYReD"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_org_repo.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "P1_numb_commits_repo/P1_numb_commits_org as P1_multi_ratio, \n",
        "P2_numb_commits_repo/P2_numb_commits_org as P2_multi_ratio,\n",
        "P3_numb_commits_repo/P3_numb_commits_org as P3_multi_ratio,\n",
        "P4_numb_commits_repo/P4_numb_commits_org as P4_multi_ratio,\n",
        "P5_numb_commits_repo/P5_numb_commits_org as P5_multi_ratio,\n",
        "P6_numb_commits_repo/P6_numb_commits_org as P6_multi_ratio\n",
        "\n",
        "FROM repos_org_repo\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oVZ5ZoLIYReE"
      },
      "source": [
        "all_table = all_table.fillna(0, subset=['P1_multi_ratio', 'P2_multi_ratio',\n",
        "                                       'P3_multi_ratio', 'P4_multi_ratio',\n",
        "                                       'P5_multi_ratio', 'P6_multi_ratio'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tVyUFnoTYReE"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_12\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpgmoMFGYReE"
      },
      "source": [
        "## 13. total_commits_org"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "p36g33D1YReE"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit\n",
        "\n",
        "FROM repos_2\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IZC0V8tOYReE"
      },
      "source": [
        "# all_table = all_table.fillna(0, subset=['total_commits_org'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SUOVqpeiYReF"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_13\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dGEXUSFaYReF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "choX14DiYReF"
      },
      "source": [
        "## 14. avg_multi_ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DaBP5VG3YReM"
      },
      "source": [
        "all_table = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_12.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "(P1_multi_ratio+P2_multi_ratio+P3_multi_ratio+P4_multi_ratio+P5_multi_ratio+P6_multi_ratio) / 6 as avg_multi_ratio\n",
        "\n",
        "FROM repos_12\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Tf4aexDYYReM"
      },
      "source": [
        "all_table = all_table.fillna(0, subset=['avg_multi_ratio'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MV24RqYOYReM"
      },
      "source": [
        "all_table.createOrReplaceTempView(\"repos_14\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "siYXeIw7YReN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTIh433OYReN"
      },
      "source": [
        "## 15. y variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uRcDG4s2YReN"
      },
      "source": [
        "all_data = spark.sql(\"\"\"\n",
        "\n",
        "SELECT repos_2.ID, \n",
        "committer_name,  committer_timestamp,  repo_names,  repo_org,  first_commit,  \n",
        "\n",
        "(CASE WHEN ((P1_numb_commits_org * 0.2) > P5_numb_commits_org AND (P1_numb_commits_org * 0.2) > P6_numb_commits_org)  \n",
        "        THEN \"disengaged\"\n",
        "        ELSE \"no_signs\" END) y\n",
        "\n",
        "FROM repos_2\n",
        "\n",
        "\"\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LlfZdnYNYReN"
      },
      "source": [
        "all_data.createOrReplaceTempView(\"repos_15\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ABZBITeYReQ"
      },
      "source": [
        "# IV - Save Tables  Locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIukpSFxYReQ"
      },
      "source": [
        "spark_repos_2 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_2\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54G7VFD4YReQ"
      },
      "source": [
        "spark_repos_3 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_3\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFL5C4kRYReQ"
      },
      "source": [
        "spark_repos_4 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_4\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-h46rcRYReQ"
      },
      "source": [
        "spark_repos_5 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_5\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6oCoRqHYReR"
      },
      "source": [
        "spark_repos_6 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_6\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwXbrTu6YReR"
      },
      "source": [
        "spark_repos_7 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_7\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgPfaMXZYReR"
      },
      "source": [
        "spark_repos_8 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_8\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPsTGmPCYReR"
      },
      "source": [
        "spark_repos_9 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_9\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xejf4CNXYReS"
      },
      "source": [
        "spark_repos_10 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_10\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQitsjuBYReS"
      },
      "source": [
        "spark_repos_11 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_11\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXRBJoUYReS"
      },
      "source": [
        "spark_repos_12 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_12\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmC-HmfhYReS"
      },
      "source": [
        "spark_repos_13 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_13\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlKhIy7oYReS"
      },
      "source": [
        "spark_repos_14 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_14\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JvSxhy-YReT"
      },
      "source": [
        "spark_repos_15 = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM  repos_15\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19uQUjNqYReT"
      },
      "source": [
        "all_table_1 = spark_repos_3.join(spark_repos_2, [\"ID\", \"committer_name\", \"committer_timestamp\",\n",
        "                                                 \"repo_names\",  \"repo_org\",  \"first_commit\" ], \"inner\")\n",
        "all_table_1.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/all_table_1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPPFdldUe3r0"
      },
      "source": [
        "spark_repos_4.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_4.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_LAvrIXe31V"
      },
      "source": [
        "spark_repos_5.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_5.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn5i7LqHe34S"
      },
      "source": [
        "spark_repos_6.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_6.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UExBHfjle37r"
      },
      "source": [
        "spark_repos_7.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_7.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n39mMx3rYReY"
      },
      "source": [
        "spark_repos_8.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_8.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h58fsapYReY"
      },
      "source": [
        "spark_repos_9.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_9.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ108R2sYReY"
      },
      "source": [
        "spark_repos_10.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_10.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTMMAc2rYReY"
      },
      "source": [
        "spark_repos_11.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_11.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddxufJUUYReX"
      },
      "source": [
        "spark_repos_12.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_12.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37FiSm3HYReY"
      },
      "source": [
        "spark_repos_13.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_13.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUrZyolPYReY"
      },
      "source": [
        "spark_repos_14.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_14.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCUTtV52YReX"
      },
      "source": [
        "spark_repos_15.toPandas().to_csv('W:/Study 2019-2020/Data Analytics/Labs/spark_repos_15.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}